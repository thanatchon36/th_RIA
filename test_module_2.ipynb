{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5babf0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network\n",
    "from pyvis.network import Network\n",
    "import matplotlib.pyplot as plt\n",
    "# Search\n",
    "import pythainlp\n",
    "from pythainlp import sent_tokenize, word_tokenize\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import re\n",
    "import itertools\n",
    "from pythainlp.corpus import thai_stopwords\n",
    "import statistics\n",
    "from colorama import Fore, Back, Style\n",
    "from statistics import StatisticsError\n",
    "import ast\n",
    "# Compare\n",
    "import pickle\n",
    "import difflib as dif\n",
    "\n",
    "class ria:\n",
    "    def __init__(self):\n",
    "        # File for Network\n",
    "        self.df_dict_pair_0 = pd.read_csv('09_Output_Streamlib/df_dict_pair.csv')\n",
    "        self.Data_Dictionary_streamlib_0 = pd.read_csv('09_Output_Streamlib/Data_Dictionary_streamlib.csv',dtype=str)\n",
    "\n",
    "        # File for Search\n",
    "        self.Doc_Page_Text_1 = pd.read_csv('09_Output_Streamlib/P_One_Doc_Page_Text.csv')\n",
    "        self.category_text_1 = pd.read_csv('09_Output_Streamlib/category_text_score.csv')\n",
    "        self.Data_Dictionary_streamlib_1 = pd.read_csv('09_Output_Streamlib/Data_Dictionary_streamlib.csv',dtype=str)\n",
    "        self.df_dict_pair_1 = pd.read_csv('09_Output_Streamlib/df_dict_pair.csv')\n",
    "\n",
    "        # File For Compare\n",
    "        self.df_dict_pair_2 = pd.read_csv('09_Output_Streamlib/df_dict_pair.csv')\n",
    "        self.Data_Dictionary_streamlib_2 = pd.read_csv('09_Output_Streamlib/Data_Dictionary_streamlib.csv',dtype=str)\n",
    "        self.Doc_Page_Text_2 = pd.read_csv('09_Output_Streamlib/P_One_Doc_Page_Text.csv')\n",
    "        self.Doc_Page_Sentence_2 = pd.read_csv('09_Output_Streamlib/Doc_Page_Sentence.csv')\n",
    "        \n",
    "    def highlight_text(self, query_sentence, text):\n",
    "        if all([True if or_token in query_sentence else False  for or_token in ['(','หรือ',')']]):\n",
    "            query_sentence = query_sentence.replace('(','')\n",
    "            query_sentence = query_sentence.replace('หรือ','')\n",
    "            query_sentence = query_sentence.replace(')','')\n",
    "            \n",
    "        query_token_list = word_tokenize(query_sentence)\n",
    "        new_query_token_list = []\n",
    "        new_token = ''\n",
    "        for index, token in enumerate(query_token_list):\n",
    "            if token in thai_stopwords():\n",
    "                new_token += token\n",
    "            elif re.search(r'([a-z])',token) != None:\n",
    "                new_query_token_list.append(token.upper())\n",
    "                new_query_token_list.append(token.lower())\n",
    "                new_query_token_list.append(token[0].upper()+token[1:].lower())\n",
    "            else:\n",
    "                new_token += token\n",
    "                new_query_token_list.append(new_token)\n",
    "                new_query_token_list.append(token)\n",
    "                new_token = ''\n",
    "                \n",
    "        token_list_to_replace = list(dict.fromkeys(new_query_token_list))\n",
    "        sorted_list = sorted(token_list_to_replace, key=len,reverse=True)\n",
    "        for index,new_token in enumerate(sorted_list):\n",
    "            if new_token != ' ':\n",
    "                text = text.replace(new_token,f'|{index}|')\n",
    "        for index,new_token in enumerate(sorted_list):\n",
    "            if new_token != ' ':\n",
    "                text = text.replace(f'|{index}|',f'<mark style=\"background-color:yellow;\">{new_token}</mark>')\n",
    "        return text\n",
    "        \n",
    "    def step3_2_click_show_result(self, query_sentence,compare_sentence):    \n",
    "        query_sentence = self.create_query_token_for_compair(query_sentence.replace('\\n',''))\n",
    "        compare_sentence = self.create_query_token_for_compair(compare_sentence.replace('\\n',''))\n",
    "        compare_sentence_result_list = list(dif.Differ().compare(query_sentence,compare_sentence))\n",
    "        \n",
    "        new_str1 = ''\n",
    "        new_str2 = ''\n",
    "        len_first = 0 #เช็กว่าเป็นคำแรกของประโยคไหม ถ้าเป็นก็จะตัดออก เพื่อปรับให้ประโยคตรงกัน\n",
    "\n",
    "        new_query_sentence = ''\n",
    "        new_compare_sentence = ''\n",
    "        '''update'''\n",
    "        compare_sentence_result_list_start = self.trim_result_list(compare_sentence_result_list)\n",
    "        compare_sentence_result_list_start.reverse()\n",
    "        compare_sentence_result_list_end = self.trim_result_list(compare_sentence_result_list_start)\n",
    "        compare_sentence_result_list_end.reverse()\n",
    "        '''update end'''\n",
    "        for symbol in compare_sentence_result_list_end:\n",
    "            if symbol[0] == ' ':\n",
    "                new_query_sentence += symbol[2:]\n",
    "                new_compare_sentence += symbol[2:]\n",
    "            elif symbol[0] == '-':\n",
    "                new_query_sentence += f\"{Fore.BLUE}{symbol[2:]}{Fore.BLACK}\" # bleu\n",
    "            elif symbol[0] == '+':\n",
    "                new_compare_sentence += f\"{Fore.RED}{symbol[2:]}{Fore.BLACK}\" # Red\n",
    "\n",
    "        return new_query_sentence.replace('BLANK',' '),new_compare_sentence.replace('BLANK',' ')\n",
    "        \n",
    "    #check if first words are differect, trim it\n",
    "    def trim_result_list(self, compare_sentence_result_list):\n",
    "        is_diff_at_start = True\n",
    "        compare_sentence_result_list_trim = []\n",
    "        for symbol in compare_sentence_result_list:\n",
    "            if symbol[0] in ['-','+'] and is_diff_at_start:\n",
    "                continue\n",
    "            else:\n",
    "                is_diff_at_start = False\n",
    "                compare_sentence_result_list_trim.append(symbol)\n",
    "        return compare_sentence_result_list_trim\n",
    "        \n",
    "\n",
    "    def part_two_show_compare(self, Doc_Page_ID):\n",
    "        df_dict_pair = self.df_dict_pair_2\n",
    "        Doc_Page_Sentence = self.Doc_Page_Sentence_2\n",
    "        Data_Dictionary_streamlib = self.Data_Dictionary_streamlib_2\n",
    "        \n",
    "        Data_Dictionary_streamlib = Data_Dictionary_streamlib[['Doc_ID','เรื่อง']].copy()\n",
    "        df_dict_pair[['Q_Doc_ID','Q_Page_ID','Q_Sen_ID']] = df_dict_pair['query'].str.split('|', expand=True)\n",
    "        df_dict_pair[['R_Doc_ID','R_Page_ID','R_Sen_ID']] = df_dict_pair['result'].str.split('|', expand=True)\n",
    "        df_dict_pair['Doc_Page_ID'] = df_dict_pair['Q_Doc_ID'] +'|'+df_dict_pair['Q_Page_ID']\n",
    "        df_dict_pair_filter = df_dict_pair[df_dict_pair['Doc_Page_ID'] == Doc_Page_ID].copy()\n",
    "        df_dict_pair_filter = df_dict_pair_filter.merge(Doc_Page_Sentence,right_on = 'Doc_Page_Sen_ID',left_on='query',how='left').drop(columns='Doc_Page_Sen_ID').rename(columns={'Sentence':'query_Sentence'})\n",
    "        df_dict_pair_filter = df_dict_pair_filter.merge(Doc_Page_Sentence,right_on = 'Doc_Page_Sen_ID',left_on='result',how='left').drop(columns='Doc_Page_Sen_ID').rename(columns={'Sentence':'result_Sentence'})\n",
    "        df_dict_pair_filter['All_Compare'] = df_dict_pair_filter.apply(lambda x: self.step3_2_click_show_result(x.query_Sentence, x.result_Sentence), axis=1)\n",
    "        split_df = pd.DataFrame(df_dict_pair_filter['All_Compare'].tolist(), columns=['query_Sentence_show','result_Sentence_show'])\n",
    "        result_all = pd.concat([df_dict_pair_filter,split_df], axis=1)\n",
    "        result_all = result_all.drop(columns=['query_Sentence','result_Sentence','All_Compare']).sort_values(by='query')\n",
    "        result_all = result_all.merge(Data_Dictionary_streamlib,right_on='Doc_ID',left_on='Q_Doc_ID',how='left')\n",
    "        result_all = result_all.rename(columns={'เรื่อง':'Q_เรื่อง'}).drop(columns=['Doc_ID'])\n",
    "        result_all = result_all.merge(Data_Dictionary_streamlib,right_on='Doc_ID',left_on='R_Doc_ID',how='left')\n",
    "        result_all = result_all.rename(columns={'เรื่อง':'R_เรื่อง'}).drop(columns=['Doc_ID'])\n",
    "        result_all.drop_duplicates(inplace=True)\n",
    "        return result_all\n",
    "        \n",
    "    def part_one_show_original_text(self, Doc_Page_ID):\n",
    "        Doc_Page_Text = self.Doc_Page_Text_2\n",
    "        Data_Dictionary_streamlib = self.Data_Dictionary_streamlib_2\n",
    "        \n",
    "        Data_Dictionary_streamlib = Data_Dictionary_streamlib[['Doc_ID','เรื่อง']].copy()\n",
    "        Doc_Page_Text[['Doc_ID','Page_ID']] = Doc_Page_Text['Doc_Page_ID'].str.split('|', expand=True)\n",
    "        df_part_one = Doc_Page_Text[Doc_Page_Text['Doc_Page_ID'] == Doc_Page_ID].merge(Data_Dictionary_streamlib,on='Doc_ID',how='left')\n",
    "        return df_part_one\n",
    "        \n",
    "    def create_query_token_for_compair(self, query):\n",
    "        query_token = word_tokenize(query)\n",
    "        return query_token\n",
    "        \n",
    "    def step1_user_search(self):\n",
    "        query = self.query\n",
    "        Doc_Page_Text = self.Doc_Page_Text_1\n",
    "        category_text = self.category_text_1\n",
    "        df_dict_pair = self.df_dict_pair_1\n",
    "        Data_Dictionary_streamlib = self.Data_Dictionary_streamlib_1\n",
    "        \n",
    "        Doc_Page_Text['Score'] = Doc_Page_Text.apply(lambda x: self.retrieval_score(x['Original_text']), axis=1)\n",
    "        Result_search = Doc_Page_Text.sort_values(by='Score', ascending=False)\n",
    "        Result_search = Result_search[Result_search['Score'] > 0]\n",
    "        Result_search['Score'] = Result_search['Score'].round(3)\n",
    "        category_score = self.create_category_score(query)\n",
    "        category_score = category_score.astype({'Category_Code': 'int'}).astype({'Category_Code': 'str'})\n",
    "        Result_search[['Doc_ID', 'Page_ID']] = Result_search['Doc_Page_ID'].str.split('|', 1, expand=True)\n",
    "        Result_search = Result_search.merge(Data_Dictionary_streamlib,on='Doc_ID',how='left')\n",
    "        Result_search = Result_search.merge(category_score,on='Category_Code',how='left')\n",
    "        Result_search = Result_search.sort_values(by=['Score','rank'],ascending=False)\n",
    "        Result_search = Result_search.drop(columns=['File_Name','Cat_Score','rank'])\n",
    "        df_dict_pair[['Q_Doc_ID','Q_Page_ID','Q_Sen_ID']] = df_dict_pair['query'].str.split('|', expand=True)\n",
    "        df_dict_pair[['R_Doc_ID','R_Page_ID','R_Sen_ID']] = df_dict_pair['result'].str.split('|', expand=True)\n",
    "        df_dict_pair['Doc_Page_ID'] = df_dict_pair['Q_Doc_ID'] + '|' + df_dict_pair['Q_Page_ID'] \n",
    "        df_dict_pair_filter_node = self.filter_node_for_search(df_dict_pair,Result_search).groupby('Doc_Page_ID')['result'].agg('count').reset_index().rename(columns={'result':'Number_result'})\n",
    "        Result_search = Result_search.merge(df_dict_pair_filter_node,on='Doc_Page_ID',how='left')\n",
    "        return Result_search\n",
    "    '''fix 20220928'''\n",
    "    def option_filter(self,Result_search):\n",
    "        filter1_from_result = list(set([i for sublist in Result_search['สถาบันผู้เกี่ยวข้อง'] for i in ast.literal_eval(sublist)]))\n",
    "        filter2_from_result = list(set([i for sublist in Result_search['ประเภทเอกสาร'] for i in ast.literal_eval(sublist)]))\n",
    "        filter3_from_result = list(set([i for sublist in Result_search['กฎหมาย'] for i in ast.literal_eval(sublist)])) \n",
    "        return filter1_from_result, filter2_from_result, filter3_from_result\n",
    "\n",
    "    def filter_col(self,row,row_names,selected_filter_lists):\n",
    "        result_each_col = []\n",
    "        y = []\n",
    "        for row_number in range(len(row_names)):\n",
    "            row_name = row_names[row_number]\n",
    "            selected_filter_list = selected_filter_lists[row_number]\n",
    "            if len(selected_filter_list) == 0:\n",
    "                result_each_col.append(True)\n",
    "            elif any([True for i in selected_filter_list if i in row[row_name]]):\n",
    "                result_each_col.append(True)\n",
    "            else:\n",
    "                result_each_col.append(False)\n",
    "        if all(result_each_col):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def filter_result_search(self,Result_search):\n",
    "        filter1_selected = self.filter1_selected\n",
    "        filter2_selected = self.filter2_selected\n",
    "        filter3_selected = self.filter3_selected\n",
    "        Result_search[\"Check\"] = Result_search.apply(self.filter_col,\n",
    "                                                     args=(['สถาบันผู้เกี่ยวข้อง','ประเภทเอกสาร','กฎหมาย'],\n",
    "                                                           [filter1_selected,filter2_selected,filter3_selected]),\n",
    "                                                     axis=1)\n",
    "        Result_search = Result_search[Result_search['Check'] == 1].copy()\n",
    "        Result_search = Result_search.drop(columns=['Check'])\n",
    "        return Result_search\n",
    "    '''end fix 20220928'''\n",
    "    def filter_node_for_search(self, df_dict_pair,Result_search):\n",
    "        Result_search_unique = Result_search['Doc_Page_ID'].unique()\n",
    "        df_dict_pair_filter = df_dict_pair[df_dict_pair['Doc_Page_ID'].isin(Result_search_unique)]\n",
    "        return df_dict_pair_filter\n",
    "        \n",
    "    def create_category_score(self, query):\n",
    "        category_text = self.category_text_1\n",
    "        query_list = self.create_query_list(query)\n",
    "        all_query_token = list(set([token for token in self.create_query_token(query) for query in query_list]))\n",
    "        try:\n",
    "            all_query_token.remove('หรือ')\n",
    "            all_query_token.remove('(')\n",
    "            all_query_token.remove(')')\n",
    "        except ValueError:\n",
    "            pass\n",
    "        filter_col = list(filter(lambda col: col in query_list , category_text.columns[1:]))\n",
    "        filter_col.append('cat')\n",
    "        df_score = pd.DataFrame(data = {'Category_Code':category_text['cat'],'Cat_Score':category_text[filter_col].sum(axis = 1)})\n",
    "        df_score = df_score.sort_values(by='Cat_Score',ascending=False)\n",
    "        df_score['rank'] = [str(i) for i in range(len(df_score)-1,-1,-1)]\n",
    "        df_score = df_score.reset_index(drop=True)\n",
    "        return df_score\n",
    "        \n",
    "    def find_min_location_token(self, document,query_token):\n",
    "        token_location_all = self.find_token_location_in_doc(document,query_token)\n",
    "        if len(query_token) > 1:\n",
    "            df = self.find_candidate_df(token_location_all)\n",
    "            candidate_df = self.find_candidate_df(token_location_all)\n",
    "            for column in df.columns[:-1]:\n",
    "                df_group = df.groupby(by=[column])['score'].agg('min').reset_index()\n",
    "                df = df.merge(df_group,how='inner', on=[column,'score'])\n",
    "        else:\n",
    "            df = self.find_candidate_df_for_len_one(token_location_all)\n",
    "            candidate_df = self.find_candidate_df_for_len_one(token_location_all)\n",
    "        return df,candidate_df\n",
    "        \n",
    "    def retrieval_score(self, document):\n",
    "        query = self.query\n",
    "        query_list = self.create_query_list(query)\n",
    "        df = pd.DataFrame()\n",
    "        candidate_df = pd.DataFrame()\n",
    "        for query in query_list:\n",
    "            query_token = self.create_query_token(query)\n",
    "            query_token = list(filter(lambda token: token != ' ', query_token))\n",
    "            try:\n",
    "                df_,candidate_df = self.find_min_location_token(document,query_token)\n",
    "                df = pd.concat([df,df_])\n",
    "            except:\n",
    "                continue\n",
    "        try:\n",
    "            word_columns = [col_word for col_word in candidate_df.columns if col_word!= 'score']\n",
    "            count = 0\n",
    "            median_score = statistics.median(df['score'])\n",
    "            for word in word_columns:\n",
    "                count += len(df[word].unique())\n",
    "            tf_score = count/len(word_columns)\n",
    "            retrieval = tf_score+(tf_score/median_score)\n",
    "            return retrieval\n",
    "        except:\n",
    "            return 0\n",
    "        \n",
    "    def find_candidate_df_for_len_one(self, token_location_all):\n",
    "        df = pd.DataFrame(data={0:token_location_all[0],'score':[1 for i in range(0,len(token_location_all[0]))]})\n",
    "        return df\n",
    "        \n",
    "    #ไม่สามารถค้นหาคำเดียวได้จึงต้องใช้find_candidate_df_for_len_one\n",
    "    def find_candidate_df(self, token_location_all):\n",
    "        candidate_element_list = []\n",
    "        for element in itertools.product(*token_location_all):\n",
    "            i = 1\n",
    "            condition_list = []\n",
    "            diff_location_element = []\n",
    "            while i < len(element):\n",
    "                if element[i] > element[i-1]:\n",
    "                    condition_list.append(True)\n",
    "                    diff_location = element[i] - element[i-1]\n",
    "                    diff_location_element.append(diff_location)\n",
    "                i = i + 1\n",
    "            if all(condition_list) and len(condition_list) == len(element)-1:\n",
    "                add_element = (element) + (sum(diff_location_element)/len(diff_location_element),)\n",
    "                candidate_element_list.append(add_element)\n",
    "        df = pd.DataFrame(candidate_element_list)\n",
    "        df = df.rename(columns={df.columns[-1]:'score'})\n",
    "        #df = df.groupby(list(df.columns)[:-1])[list(df.columns)[-1:][0]].agg('min').reset_index().drop_duplicates(subset=0,keep='first')\n",
    "        return df\n",
    "        \n",
    "    def find_token_location_in_doc(self, document,query_token):\n",
    "        token_location_all = []\n",
    "        for token in query_token:\n",
    "            location_token_list = []\n",
    "            for location_token in re.finditer(token.upper(), document.upper()):\n",
    "                 location_token_list.append(location_token.span()[1])\n",
    "            token_location_all.append(location_token_list)\n",
    "        return token_location_all\n",
    "        \n",
    "    def create_query_token(self, query):\n",
    "        query_token = word_tokenize(query)\n",
    "        stopwords = list(thai_stopwords())\n",
    "        query_token = [i for i in query_token if i not in stopwords]\n",
    "        return query_token\n",
    "        \n",
    "    def create_query_list(self, query):\n",
    "        open_bracket_location = []\n",
    "        close_bracket_location = []\n",
    "        for bracket in re.finditer('\\(', query):\n",
    "            open_bracket_location.append(bracket.span()[1])\n",
    "        for bracket in re.finditer('\\)', query):\n",
    "            close_bracket_location.append(bracket.span()[0])\n",
    "        pair_bracket_location = []\n",
    "        if len(open_bracket_location) == len(close_bracket_location):\n",
    "            for index in range(len(open_bracket_location)):\n",
    "                pair_bracket_location.append((open_bracket_location[index],close_bracket_location[index]))\n",
    "\n",
    "        query_split_all = []\n",
    "        for pair in pair_bracket_location:\n",
    "            query_splits = query[pair[0]:pair[1]].split('หรือ')\n",
    "            query_split_all.append(query_splits)\n",
    "\n",
    "        reword_list = []\n",
    "        for index in range(len(pair_bracket_location)-1, -1, -1):\n",
    "            pair_bracket = pair_bracket_location[index]\n",
    "            replace_word = query[pair_bracket[0]-1:pair_bracket[1]+1]\n",
    "            reword_list.append(f'REWORD_{index}')\n",
    "            query = query.replace(replace_word,f'REWORD_{index}')\n",
    "        reword_list = sorted(reword_list)\n",
    "\n",
    "        query_list = []\n",
    "        replcae_query = query\n",
    "        for element in itertools.product(*query_split_all):\n",
    "            replcae_query = query\n",
    "            for index_element in range(len(element)):\n",
    "                replcae_query = replcae_query.replace(reword_list[index_element],element[index_element])\n",
    "            query_list.append(replcae_query)\n",
    "        return query_list\n",
    "        \n",
    "    def filter_node(self, df_dict_pair,Result_search):\n",
    "        Result_search_unique = Result_search['Doc_Page_ID'].unique()\n",
    "        df_dict_pair_filter = df_dict_pair[df_dict_pair['Doc_Page_ID'].isin(Result_search_unique)]\n",
    "        df_dict_pair_filter_no_pair = pd.DataFrame(data={'Doc_Page_ID':list(set(Result_search_unique).difference(set(df_dict_pair['Doc_Page_ID'])))})\n",
    "        df_dict_pair_filter_no_pair[['Q_Doc_ID','Q_Page_ID']] = df_dict_pair_filter_no_pair['Doc_Page_ID'].str.split('|', expand=True)\n",
    "        return df_dict_pair_filter,df_dict_pair_filter_no_pair\n",
    "\n",
    "    #500 * 2800\n",
    "    def create_network(self, Result_search):\n",
    "        df_dict_pair = self.df_dict_pair_0\n",
    "        Data_Dictionary_streamlib = self.Data_Dictionary_streamlib_0        \n",
    "        df_dict_pair[['Q_Doc_ID','Q_Page_ID','Q_Sen_ID']] = df_dict_pair['query'].str.split('|', expand=True)\n",
    "        df_dict_pair[['R_Doc_ID','R_Page_ID','R_Sen_ID']] = df_dict_pair['result'].str.split('|', expand=True)\n",
    "        df_dict_pair['Doc_Page_ID'] = df_dict_pair['Q_Doc_ID'] + '|' + df_dict_pair['Q_Page_ID'] \n",
    "        df_dict_pair_filter ,df_dict_pair_filter_no_pair = self.filter_node(df_dict_pair,Result_search)\n",
    "\n",
    "        all_pair_Doc_id = df_dict_pair_filter[['Q_Doc_ID','R_Doc_ID']].copy()\n",
    "        all_pair_Doc_id['Count'] = 1\n",
    "        all_pair_Doc_id_group = all_pair_Doc_id.groupby(['Q_Doc_ID','R_Doc_ID'])['Count'].agg('count').reset_index()\n",
    "        #print(all_pair_Doc_id_group)\n",
    "        median_score = statistics.median(all_pair_Doc_id_group['Count'])\n",
    "        all_node = list(all_pair_Doc_id_group['Q_Doc_ID'].unique())\n",
    "        all_node.extend(all_pair_Doc_id_group['R_Doc_ID'].unique())\n",
    "        all_node = list(set(all_node))\n",
    "        G = Network(height='500px', width='100%',bgcolor=\"#f2f2f2\")  #222222\n",
    "\n",
    "        for Doc_ID in df_dict_pair_filter_no_pair['Q_Doc_ID'].unique():\n",
    "            Doc_Name = Data_Dictionary_streamlib[Data_Dictionary_streamlib['Doc_ID'] == Doc_ID]['เรื่อง'].iloc[0]\n",
    "            G.add_node(Doc_ID,title=[\"ประกาศหลัก:\"+'\\n'+Doc_ID+' :'+Doc_Name],shape='dot',size =30) #circle\n",
    "\n",
    "        for Doc_ID in all_node:\n",
    "            Doc_Name = Data_Dictionary_streamlib[Data_Dictionary_streamlib['Doc_ID'] == Doc_ID]['เรื่อง'].iloc[0]\n",
    "    #         Doc_ID_Name_len = len(Doc_Name)\n",
    "    #         if Doc_ID_Name_len > 100:\n",
    "    #             Doc_Name = Doc_Name[:round(Doc_ID_Name_len/2)]+'\\n'+Doc_Name[round(Doc_ID_Name_len/2):]\n",
    "            G.add_node(Doc_ID,title=[\"ประกาศหลัก:\"+'\\n'+Doc_ID+' :'+Doc_Name],shape='dot',size =30)\n",
    "        try:\n",
    "            for Q_Doc_ID in all_pair_Doc_id_group['Q_Doc_ID'].unique():\n",
    "                Number_connect_nodes = len(all_pair_Doc_id_group[all_pair_Doc_id_group['Q_Doc_ID'] == Q_Doc_ID])\n",
    "                for Number_connect_node in range(Number_connect_nodes):\n",
    "                    R_Doc_ID = all_pair_Doc_id_group[all_pair_Doc_id_group['Q_Doc_ID'] == Q_Doc_ID]['R_Doc_ID'].iloc[Number_connect_node]\n",
    "                    weight = all_pair_Doc_id_group[all_pair_Doc_id_group['Q_Doc_ID'] == Q_Doc_ID]['Count'].iloc[Number_connect_node]\n",
    "                    if weight > median_score:\n",
    "                        value = 64\n",
    "                    else:\n",
    "                        value = 56\n",
    "                    G.add_edge(str(Q_Doc_ID), str(R_Doc_ID), value=str(value),title='จำนวนคู่ที่เหมือนกัน:'+str(weight))\n",
    "            neighbor_map = G.get_adj_list()               \n",
    "            for node in G.nodes:\n",
    "                if len(neighbor_map[node['id']]) >=1:\n",
    "                    node['title'][0] += '\\n\\n ประกาศที่เกี่ยวข้อง:\\n'\n",
    "                #print(node,node['size'])\n",
    "                for Doc_ID in neighbor_map[node['id']]:\n",
    "                    Doc_ID_Name = Data_Dictionary_streamlib[Data_Dictionary_streamlib['Doc_ID'] == Doc_ID]['เรื่อง'].iloc[0]\n",
    "                    node['title'][0] += f' {Doc_ID} :'+Doc_ID_Name+ '\\n'\n",
    "        except:\n",
    "            pass\n",
    "    #     \"border\": \"rgba(34, 42, 89,1)\",\n",
    "    #     \"background\": \"rgba(11, 81, 89,1)\",\n",
    "        G.set_options('''\n",
    "    var options = {\n",
    "      \"nodes\": {\n",
    "        \"borderWidth\": 1,\n",
    "        \"borderWidthSelected\": 1,\n",
    "        \"font\": {\n",
    "          \"color\": \"rgba(114, 114, 115,1)\",\n",
    "          \"size\": 25\n",
    "          },\n",
    "        \"color\": {\n",
    "          \"border\": \"rgba(123, 149, 166,1)\",\n",
    "          \"background\": \"rgba(22, 79, 115,1)\",\n",
    "          \"highlight\": {\n",
    "            \"border\": \"rgba(22, 79, 115,1)\",\n",
    "            \"background\": \"rgba(107, 204, 242,1)\"\n",
    "          },\n",
    "          \"hover\": {\n",
    "            \"border\": \"rgba(22, 79, 115,1)\",\n",
    "            \"background\": \"rgba(107, 204, 242,1)\"\n",
    "          }\n",
    "        }\n",
    "      },\n",
    "      \"edges\": {\n",
    "        \"color\": {\n",
    "          \"color\": \"rgba(114, 114, 115,1)\",\n",
    "          \"hover\": \"rgba(142, 191, 107,1)\",\n",
    "          \"inherit\": false\n",
    "        },\n",
    "        \"font\": {\n",
    "          \"align\": \"middle\"\n",
    "        },\n",
    "        \"hoverWidth\": 3.1,\n",
    "        \"smooth\": false\n",
    "      },\n",
    "      \"interaction\": {\n",
    "        \"hover\": true\n",
    "      },\n",
    "      \"physics\": {\n",
    "        \"barnesHut\": {\n",
    "          \"gravitationalConstant\": -59050,\n",
    "          \"centralGravity\": 1.75,\n",
    "          \"springLength\": 45,\n",
    "          \"springConstant\": 0.001\n",
    "        },\n",
    "        \"minVelocity\": 0.75\n",
    "      }\n",
    "    }\n",
    "    ''')\n",
    "        return G        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50172442",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = ria()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d14e63ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "app.query = 'ธนาคาร'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4da34ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Result_search  = app.step1_user_search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b79e1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter1_from_result, filter2_from_result, filter3_from_result = app.option_filter(Result_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3526afb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['สถาบันการเงินเฉพาะกิจ',\n",
       " 'ผู้ให้บริการชำระเงินทางอิเล็กทรอนิกส์ที่มิใช่ สง.',\n",
       " 'ธนาคารพาณิชย์จดทะเบียนในประเทศ',\n",
       " 'บริษัทเงินทุน',\n",
       " 'ผู้ประกอบธุรกิจบัตรเครดิต',\n",
       " 'อื่น ๆ',\n",
       " 'บริษัทเครดิตฟองซิเอร์',\n",
       " 'บริษัทบริหารสินทรัพย์',\n",
       " 'บ.ที่ประกอบธุรกิจสินเชื่อส่วนบุคคลภายใต้การกำกับ',\n",
       " 'ธนาคารพาณิชย์ที่จดทะเบียนในต่างประเทศ',\n",
       " 'บริษัทผู้ประกอบธุรกิจสินเชื่อรายย่อยเพื่อการประกอบอาชีพภายใต้การกำกับ']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter1_from_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5d61468",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['สถาบันการเงินเฉพาะกิจ',\n",
       " 'ผู้ให้บริการชำระเงินทางอิเล็กทรอนิกส์ที่มิใช่ สง.',\n",
       " 'ธนาคารพาณิชย์จดทะเบียนในประเทศ',\n",
       " 'บริษัทเงินทุน',\n",
       " 'ผู้ประกอบธุรกิจบัตรเครดิต',\n",
       " 'อื่น ๆ',\n",
       " 'บริษัทเครดิตฟองซิเอร์',\n",
       " 'บริษัทบริหารสินทรัพย์',\n",
       " 'บ.ที่ประกอบธุรกิจสินเชื่อส่วนบุคคลภายใต้การกำกับ',\n",
       " 'ธนาคารพาณิชย์ที่จดทะเบียนในต่างประเทศ',\n",
       " 'บริษัทผู้ประกอบธุรกิจสินเชื่อรายย่อยเพื่อการประกอบอาชีพภายใต้การกำกับ']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter1_from_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055544de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abb96d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22d915ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['หนังสือเวียน', 'อื่น ๆ', 'ประกาศ ธปท.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter2_from_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe60996d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['พรบ.สง. มาตรา  30',\n",
       " 'พรบ.สง. มาตรา  80',\n",
       " 'พรบ.สง. มาตรา  13',\n",
       " 'พรบ.สง. มาตรา  44',\n",
       " 'พรบ.สง. มาตรา  57']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter3_from_result[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b9d736c",
   "metadata": {},
   "outputs": [],
   "source": [
    "app.filter1_selected, app.filter2_selected, app.filter3_selected = ['สถาบันการเงินเฉพาะกิจ'],[],[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09529aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = app.filter_result_search(Result_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1934ed80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['บริษัทเครดิตฟองซิเอร์', 'ธนาคารพาณิชย์ที่จดทะเบียนในต่างประเทศ', 'ธนาคารพาณิชย์จดทะเบียนในประเทศ', 'บริษัทเงินทุน', 'สถาบันการเงินเฉพาะกิจ']\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df['สถาบันผู้เกี่ยวข้อง'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db713ee5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec55b8e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py39thria]",
   "language": "python",
   "name": "conda-env-py39thria-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
